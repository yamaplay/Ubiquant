{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgbm\nfrom lightgbm import *","metadata":{"execution":{"iopub.status.busy":"2022-04-15T08:24:38.051073Z","iopub.execute_input":"2022-04-15T08:24:38.051422Z","iopub.status.idle":"2022-04-15T08:24:40.144576Z","shell.execute_reply.started":"2022-04-15T08:24:38.051324Z","shell.execute_reply":"2022-04-15T08:24:40.143752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"既に多くのBaseline、EDAが有志の方々によって挙げられていたため、そこは省略します。\n今回は初めてのkaggleということで、特徴量を１つ作成した上でLGBMを利用した予測を行いました。","metadata":{}},{"cell_type":"markdown","source":"investment_numはtime_idごとのinvestment_idの数の合計を当てはめたもので、investment_idの数が多い時期ほど投資の背景に何らかの事象が発生していると考えられ、予測に活用できる、との指摘がありました。\nhttps://qiita.com/tmrtj9999/items/15abb2ba3a6ea124dcbc\nこれを参考に特徴量を作成しました。本家ではdatesetの作り方までは公開されていなかったので、独自にやり方を再現しました。\n","metadata":{}},{"cell_type":"markdown","source":"当コンペでは、使えるRAMがUbiquantデータセットを読み込めるほどの大きさではありません。そこで、何らかの形でファイルを圧縮する必要があります。\nここではpickleという形式を使い、データを圧縮します。作り方は簡単で、df.to_pickleを利用するだけです。\nデータにはsupplemental_train.csvが含まれており、これは追加のデータです。このデータと先述の特徴量を加えたpklを独自に作成しました。\nデータ圧縮の方法はもう一つあり、その場合はint,float型を64から下げることです。その場合はdef reduce_mamory_usageなどを作成してください。","metadata":{}},{"cell_type":"code","source":"df = pd.read_pickle(\"../input/low-mem-investment/train_num_investment.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-04-15T08:32:39.19233Z","iopub.execute_input":"2022-04-15T08:32:39.192651Z","iopub.status.idle":"2022-04-15T08:33:10.731256Z","shell.execute_reply.started":"2022-04-15T08:32:39.192605Z","shell.execute_reply":"2022-04-15T08:33:10.730499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T08:33:22.899274Z","iopub.execute_input":"2022-04-15T08:33:22.899918Z","iopub.status.idle":"2022-04-15T08:33:22.943648Z","shell.execute_reply.started":"2022-04-15T08:33:22.89988Z","shell.execute_reply":"2022-04-15T08:33:22.942871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = df.columns\nprint(a)\n\nb = df[\"target\"].max()\nprint(b)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T08:37:50.277812Z","iopub.execute_input":"2022-04-15T08:37:50.278328Z","iopub.status.idle":"2022-04-15T08:37:50.36029Z","shell.execute_reply.started":"2022-04-15T08:37:50.278296Z","shell.execute_reply":"2022-04-15T08:37:50.359428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\n\n\nfeatures = [f'f_{i}' for i in range(300)] + [\"num_investment\"]\ntarget = 'target'\n \n\ndf_features = df[features]\n\n\nX_train, X_val, Y_train, Y_val = train_test_split(df_features, df[target], train_size=0.80, shuffle=False)\n\ndel df\n\ndf = [[]]\ndf_features = [[]]","metadata":{"execution":{"iopub.status.busy":"2022-04-13T11:20:28.108021Z","iopub.execute_input":"2022-04-13T11:20:28.108256Z","iopub.status.idle":"2022-04-13T11:21:03.508212Z","shell.execute_reply.started":"2022-04-13T11:20:28.108229Z","shell.execute_reply":"2022-04-13T11:21:03.507146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"paramの値はランダムです。ベイズ最適化など、もっと良い方法を利用できると思います。","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\nwarnings.simplefilter('ignore')\n\nlgb_train = lgb.Dataset(X_train, Y_train)\nlgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n\nparams = {\n        \"objective\": \"regression\",\n        \"num_leaves\": 14,\n        \"max_depth\": 4,\n        \"metric\" : \"rmse\",\n        \"feature_fraction\": 0.8,\n        \"subsample_freq\": 1,\n        \"bagging_fraction\": 0.7,\n        \"min_data_in_leaf\": 10,\n        \"learning_rate\": 0.2,\n        \"boosting\": \"gbdt\",\n        \"lambda_l1\": 0.4,\n        \"lambda_l2\": 0.4,\n        \"random_state\": 42,\n}\n        \n        \ngbm = lgb.train(params,\n                lgb_train,\n                valid_sets=lgb_eval,\n                num_boost_round =3000,\n                early_stopping_rounds=3\n                )","metadata":{"execution":{"iopub.status.busy":"2022-04-13T11:21:03.510116Z","iopub.execute_input":"2022-04-13T11:21:03.510911Z","iopub.status.idle":"2022-04-13T11:21:03.520654Z","shell.execute_reply.started":"2022-04-13T11:21:03.510875Z","shell.execute_reply":"2022-04-13T11:21:03.519143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    \n    \n    test_df.drop(['row_id'], axis=1, inplace=True)\n    test_df.drop(['investment_id'], axis=1, inplace=True)\n    test_df['num_investment'] = len(test_df)\n    pred = gbm.predict(test_df)\n    sample_prediction_df['target'] = pred\n    env.predict(sample_prediction_df)   # register predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-13T11:21:03.522016Z","iopub.status.idle":"2022-04-13T11:21:03.522381Z","shell.execute_reply.started":"2022-04-13T11:21:03.522196Z","shell.execute_reply":"2022-04-13T11:21:03.522215Z"},"trusted":true},"execution_count":null,"outputs":[]}]}